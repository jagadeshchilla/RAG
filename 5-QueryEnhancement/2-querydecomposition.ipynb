{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### üß† What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### ‚úÖ Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Welcome\\OneDrive\\Documents\\AI\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000025600274470>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256001B9160>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:openai/gpt-oss-120b\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sub‚Äëquestions**\n",
      "\n",
      "1. What memory architectures and capabilities does LangChain offer for managing conversational context?  \n",
      "2. How are agents designed and used within the LangChain framework (e.g., tool‚Äëcalling, routing, planning)?  \n",
      "3. How does CrewAI implement memory handling and agent orchestration in its workflow?  \n",
      "4. In what ways do LangChain‚Äôs memory and agent approaches differ from or resemble those of CrewAI (e.g., architecture, flexibility, scalability, integration with external tools)?\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-‚Ä¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final Answer:\n",
      "\n",
      "Q: **Sub‚Äëquestions**\n",
      "A: Below are several concrete sub‚Äëquestions you can ask to explore the concepts mentioned in the context more deeply:\n",
      "\n",
      "1. **Knowledge Injection**\n",
      "   - How does injecting external knowledge into the LLM prompt reduce hallucinations?\n",
      "   - What mechanisms does CrewAI use to fetch, validate, and inject knowledge into prompts?\n",
      "   - Are there best‚Äëpractice patterns for formatting injected knowledge (e.g., JSON, bullet lists, tables)?\n",
      "\n",
      "2. **Agent Context‚ÄëSharing in CrewAI**\n",
      "   - What data structures are used to pass intermediate results between agents?\n",
      "   - How does CrewAI decide which agent should handle a given piece of context (delegation logic)?\n",
      "   - Can agents request clarification or additional data from one another (consultation flow)?\n",
      "   - What safeguards exist to prevent circular dependencies or infinite loops in context‚Äësharing?\n",
      "\n",
      "3. **Emergent Behaviors (Delegation, Consultation, Review)**\n",
      "   - How does an agent recognize that a task is beyond its competence and trigger delegation?\n",
      "   - In what ways can agents ‚Äúreview‚Äù each other‚Äôs outputs, and how is consensus reached?\n",
      "   - Are there configurable thresholds (confidence scores, token limits) that trigger these emergent behaviors?\n",
      "\n",
      "4. **Prompt Engineering in LangChain**\n",
      "   - How do LangChain‚Äôs templating features support dynamic insertion of injected knowledge?\n",
      "   - What are the advantages of nesting prompt templates versus flat templates?\n",
      "   - How can input variables be reused across multiple chains without redundancy?\n",
      "\n",
      "5. **Scalability (Horizontal vs. Vertical)**\n",
      "   - What architectural patterns enable horizontal scaling (adding more agents) in CrewAI?\n",
      "   - How does increasing reasoning depth (vertical scaling) affect latency and token usage?\n",
      "   - Are there recommended strategies for balancing horizontal and vertical scaling for a given workload?\n",
      "\n",
      "6. **Practical Implementation**\n",
      "   - Can you provide a minimal code example that demonstrates knowledge injection, context‚Äësharing, and a delegated task across two agents?\n",
      "   - How do you monitor and debug the flow of intermediate data between agents in a production environment?\n",
      "   - What logging or observability tools integrate well with CrewAI for tracking agent interactions?\n",
      "\n",
      "7. **Comparison with Other Frameworks**\n",
      "   - How does CrewAI‚Äôs context‚Äësharing differ from the ‚Äúmemory‚Äù mechanisms in other multi‚Äëagent frameworks (e.g., Auto‚ÄëGPT, BabyAGI)?\n",
      "   - In what scenarios would you prefer LangChain‚Äôs prompt chaining over CrewAI‚Äôs agent collaboration, and vice versa?\n",
      "\n",
      "8. **Limitations & Future Directions**\n",
      "   - What are the current limitations of knowledge injection (e.g., size limits, stale data)?\n",
      "   - How might future versions of CrewAI improve the robustness of emergent behaviors like review and consultation?\n",
      "   - Are there plans to incorporate retrieval‚Äëaugmented generation (RAG) pipelines directly into the context‚Äësharing layer?\n",
      "\n",
      "Feel free to pick any of the above sub‚Äëquestions (or modify them) to dive deeper into the topics you‚Äôre most interested in!\n",
      "\n",
      "Q: What memory mechanisms does LangChain provide, and how are they used within LangChain‚Äëbased applications?\n",
      "A: **LangChain‚Äôs built‚Äëin memory mechanisms**\n",
      "\n",
      "LangChain supplies a set of ready‚Äëto‚Äëuse ‚Äúmemory‚Äù classes that let an LLM recall information from previous interactions (or from external sources) without having to re‚Äëinject the whole history into every prompt.  The most common ones are:\n",
      "\n",
      "| Memory class | What it stores | Typical use‚Äëcase | How you plug it into a chain / agent |\n",
      "|--------------|----------------|------------------|--------------------------------------|\n",
      "| **ConversationBufferMemory** | Raw list of past user‚Äëassistant messages (chronological). | Simple chat‚Äëstyle bots where you want the full dialogue context. | `memory = ConversationBufferMemory()` ‚Üí pass `memory=memory` to `LLMChain` or an `AgentExecutor`. |\n",
      "| **ConversationBufferWindowMemory** | A sliding window of the last *k* turns (default k=2). | When the full history is too long for the model‚Äôs context window. | `memory = ConversationBufferWindowMemory(k=3)` |\n",
      "| **ConversationSummaryMemory** | A running summary of the conversation, generated by the LLM itself. | Long‚Äërunning assistants where you need a compact representation of what has happened so far. | `memory = ConversationSummaryMemory(llm=ChatOpenAI(...))` |\n",
      "| **ConversationSummaryBufferMemory** | Combination of a summary plus the most recent *k* raw turns. | Keeps a concise ‚Äúbig picture‚Äù while still showing the latest details. | `memory = ConversationSummaryBufferMemory(llm=..., k=2)` |\n",
      "| **VectorStoreRetrieverMemory** | Stores embeddings of past texts in a vector store (FAISS, Chroma, Pinecone, etc.) and retrieves the most relevant chunks on demand. | Retrieval‚Äëaugmented generation (RAG) where the LLM should recall specific facts or documents. | ```python\\nvectorstore = FAISS.from_texts(past_texts, embedding)\\nretriever = vectorstore.as_retriever(search_kwargs={\\\"k\\\": 3})\\nmemory = VectorStoreRetrieverMemory(retriever=retriever)\\n``` |\n",
      "| **CombinedMemory** | Chains multiple memory objects together (e.g., buffer + vector store). | When you need both short‚Äëterm dialogue context **and** long‚Äëterm document recall. | `memory = CombinedMemory(memories=[ConversationBufferMemory(), VectorStoreRetrieverMemory(... )])` |\n",
      "| **ConversationTokenBufferMemory** | Keeps the most recent turns that fit within a token budget rather than a fixed turn count. | Useful when you are close to the model‚Äôs max token limit and need a dynamic cut‚Äëoff. | `memory = ConversationTokenBufferMemory(max_token_limit=1500, llm=ChatOpenAI(...))` |\n",
      "| **EntityMemory** (experimental) | Tracks named entities (people, places, objects) mentioned in the conversation and stores attributes about them. | Complex agents that need to maintain a ‚Äúworld model‚Äù (e.g., RPG NPCs, personal assistants). | `memory = EntityMemory(llm=ChatOpenAI(...))` |\n",
      "| **ConversationSummaryMemory with custom prompt** | Same as `ConversationSummaryMemory` but you can supply your own prompt template for the summarizer. | When the default summarizer is not appropriate for your domain. | `memory = ConversationSummaryMemory(llm=..., summary_prompt=PromptTemplate(...))` |\n",
      "\n",
      "---\n",
      "\n",
      "### How memory is used inside a LangChain‚Äëbased application\n",
      "\n",
      "1. **Create the memory object**  \n",
      "   ```python\n",
      "   from langchain.memory import ConversationBufferMemory\n",
      "   memory = ConversationBufferMemory()\n",
      "   ```\n",
      "\n",
      "2. **Attach it to a chain (or an agent)**  \n",
      "   Most LangChain chain classes accept a `memory=` argument.  \n",
      "   ```python\n",
      "   from langchain.chains import LLMChain\n",
      "   from langchain.prompts import PromptTemplate\n",
      "   from langchain.llms import ChatOpenAI\n",
      "\n",
      "   prompt = PromptTemplate(\n",
      "       input_variables=[\"history\", \"input\"],\n",
      "       template=\"{history}\\nHuman: {input}\\nAssistant:\"\n",
      "   )\n",
      "   chain = LLMChain(\n",
      "       llm=ChatOpenAI(model=\"gpt-4\"),\n",
      "       prompt=prompt,\n",
      "       memory=memory\n",
      "   )\n",
      "   ```\n",
      "\n",
      "   The chain will automatically:\n",
      "   * **Load** the stored context (`memory.load_memory_variables({})`) before each LLM call.\n",
      "   * **Append** the new user‚Äëassistant turn to the memory after the LLM returns (`memory.save_context(inputs, outputs)`).\n",
      "\n",
      "3. **Running the chain**  \n",
      "   ```python\n",
      "   response = chain.run({\"input\": \"What did we talk about earlier?\"})\n",
      "   print(response)\n",
      "   ```\n",
      "\n",
      "   Internally, the chain does roughly:\n",
      "   ```python\n",
      "   # 1Ô∏è‚É£ Retrieve stored context\n",
      "   history = memory.load_memory_variables({})[\"history\"]\n",
      "\n",
      "   # 2Ô∏è‚É£ Fill the prompt template\n",
      "   full_prompt = prompt.format(history=history, input=user_input)\n",
      "\n",
      "   # 3Ô∏è‚É£ Call the LLM\n",
      "   answer = llm(full_prompt)\n",
      "\n",
      "   # 4Ô∏è‚É£ Save the turn\n",
      "   memory.save_context({\"input\": user_input}, {\"output\": answer})\n",
      "   ```\n",
      "\n",
      "4. **Using memory with agents**  \n",
      "   Agents (e.g., `ZeroShotAgent`, `ToolCallingAgent`) also accept a `memory=` argument.  \n",
      "   ```python\n",
      "   from langchain.agents import initialize_agent, Tool\n",
      "   agent = initialize_agent(\n",
      "       tools=[search_tool],\n",
      "       llm=ChatOpenAI(),\n",
      "       agent_type=\"zero-shot-react-description\",\n",
      "       memory=ConversationSummaryMemory(llm=ChatOpenAI())\n",
      "   )\n",
      "   ```\n",
      "\n",
      "   The agent will keep a running summary (or any other memory you supplied) and automatically inject it into the prompt it builds for each step.\n",
      "\n",
      "5. **Persisting memory across sessions**  \n",
      "   Some memory classes can be backed by a persistent store (e.g., a vector DB, a file‚Äëbased SQLite store).  \n",
      "   ```python\n",
      "   from langchain.memory import ConversationBufferMemory\n",
      "   memory = ConversationBufferMemory(return_messages=True)  # returns Message objects\n",
      "   # optionally serialize to JSON or a DB after each turn\n",
      "   ```\n",
      "\n",
      "   For long‚Äëlived assistants you typically:\n",
      "   * Save the memory state after each interaction (`memory.save_context(...)` already updates the in‚Äëmemory object).\n",
      "   * Serialize it (`memory.save_context(...); json.dump(memory.buffer, f)`) or rely on a vector‚Äëstore that persists on disk/cloud.\n",
      "\n",
      "---\n",
      "\n",
      "### Choosing the right memory for your app\n",
      "\n",
      "| Scenario | Recommended memory | Why |\n",
      "|----------|--------------------|-----|\n",
      "| **Simple chatbot** | `ConversationBufferMemory` or `ConversationBufferWindowMemory` | Easy, keeps raw dialogue. |\n",
      "| **Long conversation, limited token budget** | `ConversationSummaryMemory` or `ConversationTokenBufferMemory` | Summarizes to stay within context limits. |\n",
      "| **Retrieval‚Äëaugmented QA** | `VectorStoreRetrieverMemory` (FAISS, Chroma, Pinecone) | Retrieves relevant documents/facts. |\n",
      "| **Hybrid: short‚Äëterm chat + long‚Äëterm knowledge** | `CombinedMemory` (buffer + vector store) | Gives both recent turns and searchable facts. |\n",
      "| **Entity‚Äërich interactions (games, personal assistants)** | `EntityMemory` | Tracks entities and their attributes across turns. |\n",
      "| **Need custom summarization** | `ConversationSummaryMemory` with a custom `summary_prompt` | Tailor the summary style to your domain. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick code snippet that demonstrates several memories together\n",
      "\n",
      "```python\n",
      "from langchain.llms import ChatOpenAI\n",
      "from langchain.memory import (\n",
      "    ConversationBufferWindowMemory,\n",
      "    VectorStoreRetrieverMemory,\n",
      "    CombinedMemory,\n",
      ")\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "# 1Ô∏è‚É£ Short‚Äëterm buffer (last 3 turns)\n",
      "buffer_mem = ConversationBufferWindowMemory(k=3)\n",
      "\n",
      "# 2Ô∏è‚É£ Long‚Äëterm vector store (pre‚Äëpopulated with docs)\n",
      "emb = OpenAIEmbeddings()\n",
      "doc_texts = [\"LangChain is a framework for LLM apps.\", \"Memory lets agents recall...\"]\n",
      "vectorstore = FAISS.from_texts(doc_texts, emb)\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
      "vector_mem = VectorStoreRetrieverMemory(retriever=retriever)\n",
      "\n",
      "# 3Ô∏è‚É£ Combine them\n",
      "memory = CombinedMemory(memories=[buffer_mem, vector_mem])\n",
      "\n",
      "# 4Ô∏è‚É£ Build a chain that uses the combined memory\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"history\", \"retrieved_docs\", \"input\"],\n",
      "    template=\"\"\"\n",
      "    Context from past conversation:\n",
      "    {history}\n",
      "    \n",
      "    Relevant documents:\n",
      "    {retrieved_docs}\n",
      "    \n",
      "    Human: {input}\n",
      "    Assistant:\"\"\",\n",
      ")\n",
      "\n",
      "chain = LLMChain(\n",
      "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
      "    prompt=prompt,\n",
      "    memory=memory,\n",
      ")\n",
      "\n",
      "# 5Ô∏è‚É£ Run\n",
      "print(chain.run({\"input\": \"Explain how LangChain memory works.\"}))\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "* `buffer_mem` supplies the recent dialogue (`{history}`).\n",
      "* `vector_mem` fetches the most relevant stored documents (`{retrieved_docs}`).\n",
      "* `CombinedMemory` merges both sources, and the chain‚Äôs prompt template can reference each piece explicitly.\n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR\n",
      "\n",
      "- **Memory classes**: buffer, windowed buffer, token‚Äëbudget buffer, summarizing buffers, vector‚Äëstore retrievers, combined, entity‚Äëtrackers, and custom‚Äëprompt summaries.\n",
      "- **How they are used**: instantiate a memory object, pass it to `LLMChain`, `AgentExecutor`, or any other LangChain component via the `memory=` argument; the framework automatically loads past context before each LLM call and saves the new turn afterward.\n",
      "- **When to pick which**: choose based on conversation length, token limits, need for retrieval‚Äëaugmented generation, or domain‚Äëspecific entity tracking.  \n",
      "\n",
      "By leveraging these built‚Äëin memories, LangChain applications can maintain coherent, context‚Äëaware interactions without manually stitching prompt histories together.\n",
      "\n",
      "Q: How does LangChain implement and employ agents for task orchestration and execution?\n",
      "A: **LangChain‚Äôs agent architecture is built around a‚ÄØ*planner‚Äëexecutor*‚ÄØloop that lets a language model act like a tiny ‚Äúorchestrator‚Äù for arbitrary tools.**  \n",
      "\n",
      "Below is a step‚Äëby‚Äëstep description of how LangChain implements and uses agents to turn a high‚Äëlevel user request into a concrete, multi‚Äëstep workflow:\n",
      "\n",
      "| Phase | What happens | How LangChain implements it |\n",
      "|-------|--------------|-----------------------------|\n",
      "| **1. Prompt ‚Üí Planner** | The user‚Äôs natural‚Äëlanguage request is fed to an LLM together with a *tool‚Äëspecification* (a list of available tools, their names, descriptions, and expected input schemas). | LangChain builds a **system prompt** that tells the LLM: *‚ÄúYou are an agent. Choose a tool, provide its arguments, and when you‚Äôre done return the final answer.‚Äù* The LLM‚Äôs output is parsed into a **plan** ‚Äì a sequence of tool‚Äëcall ‚Äúactions‚Äù (e.g., `search(query=\"‚Ä¶\")`, `calc(expression=\"‚Ä¶\")`, `python(code=\"‚Ä¶\")`). |\n",
      "| **2. Planner ‚Üí Executor** | Each action from the plan is executed one at a time. The result of one step can be fed back into the next step, enabling branching and dynamic decision‚Äëmaking. | LangChain provides an **Executor** that iterates over the plan: <br>‚ÄØ‚ÄØ‚Ä¢ Looks up the requested tool in a registry (`tool_name ‚Üí callable`). <br>‚ÄØ‚ÄØ‚Ä¢ Validates the arguments (using the schema supplied in the tool spec). <br>‚ÄØ‚ÄØ‚Ä¢ Calls the tool (e.g., makes a web request, runs a calculator, executes Python code). <br>‚ÄØ‚ÄØ‚Ä¢ Captures the output and stores it in the agent‚Äôs **memory**. |\n",
      "| **3. Memory & Context** | The LLM may need information from previous steps to decide the next action (e.g., ‚Äúthe search returned no results, try a different query‚Äù). | LangChain attaches a **memory buffer** (simple list, summary buffer, vector store, etc.) to the agent. After each tool call the output is appended to the buffer, and the buffer is re‚Äëincluded in the next LLM prompt so the model has full context of what has already happened. |\n",
      "| **4. Dynamic Branching** | If a tool‚Äôs result indicates that the original plan is no longer optimal, the LLM can generate a new sub‚Äëplan on‚Äëthe‚Äëfly. | Because the planner is invoked **after every step** (or whenever the LLM signals ‚Äúneed more info‚Äù), the agent can re‚Äëplan: the LLM sees the latest memory, decides whether to continue the current sequence or start a new branch, and emits a fresh set of actions. |\n",
      "| **5. Final Output** | Once the LLM decides the goal is satisfied, it returns the final answer to the user. | The executor detects a special ‚Äúfinal answer‚Äù token (e.g., `Final Answer:`) in the LLM‚Äôs output and stops the loop, returning that text to the caller. |\n",
      "\n",
      "### Core Components in LangChain Code\n",
      "\n",
      "| Component | Role | Typical Class/Function |\n",
      "|-----------|------|------------------------|\n",
      "| **Agent** | Orchestrates the planner‚Äëexecutor loop; holds references to tools, memory, and the LLM. | `AgentExecutor`, `ZeroShotAgent`, `ConversationalAgent` |\n",
      "| **Planner (LLM)** | Generates the next action(s) based on the current prompt + memory. | Any `LLM` wrapper (`ChatOpenAI`, `AzureChatOpenAI`, etc.) used with a **prompt template** that includes the tool list. |\n",
      "| **Tool Registry** | Maps tool names to callables and defines input schemas for validation. | `Tool`, `StructuredTool`, `BaseTool` subclasses (e.g., `SearchTool`, `Calculator`, `PythonREPLTool`). |\n",
      "| **Executor** | Calls the selected tool, captures output, updates memory, and decides whether to continue. | Inside `AgentExecutor.run()` ‚Äì the loop that parses `action = tool.run(input)` and feeds back the result. |\n",
      "| **Memory** | Persists intermediate results so the LLM can reason across steps. | `ConversationBufferMemory`, `ConversationSummaryMemory`, `VectorStoreRetrieverMemory`, etc. |\n",
      "| **Prompt Templates** | Provide the LLM with the ‚Äúagent instruction set‚Äù and the list of tools. | `ZeroShotAgent.create_prompt(tools, ...)`, `ChatPromptTemplate` with `SystemMessagePromptTemplate`. |\n",
      "\n",
      "### Typical Workflow (pseudo‚Äëcode)\n",
      "\n",
      "```python\n",
      "# 1. Define the tools the agent may use\n",
      "tools = [\n",
      "    Tool(name=\"search\", func=web_search, description=\"Search the web for a query.\"),\n",
      "    Tool(name=\"calc\",   func=lambda expr: str(eval(expr)), description=\"Simple calculator.\"),\n",
      "    Tool(name=\"python\", func=python_repl, description=\"Execute Python code.\")\n",
      "]\n",
      "\n",
      "# 2. Choose an LLM and create the planner prompt\n",
      "llm = ChatOpenAI(model=\"gpt-4o\")\n",
      "agent_prompt = ZeroShotAgent.create_prompt(tools, prefix=PREFIX, suffix=SUFFIX)\n",
      "\n",
      "# 3. Assemble the agent with memory\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "agent = AgentExecutor.from_agent_and_tools(\n",
      "    agent=ZeroShotAgent(llm=llm, prompt=agent_prompt),\n",
      "    tools=tools,\n",
      "    memory=memory,\n",
      "    max_iterations=10\n",
      ")\n",
      "\n",
      "# 4. Run a user query\n",
      "result = agent.run(\"Find the latest COVID‚Äë19 statistics for Italy and compute the 7‚Äëday average.\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "During `agent.run()`:\n",
      "\n",
      "1. The LLM receives the user query + the tool list + any prior memory.\n",
      "2. It replies with something like:  \n",
      "   ```\n",
      "   Action: search\n",
      "   Action Input: \"latest COVID-19 Italy statistics\"\n",
      "   ```\n",
      "3. The executor calls `web_search(...)`, stores the HTML snippet in memory.\n",
      "4. The next LLM prompt now includes that snippet and decides:  \n",
      "   ```\n",
      "   Action: python\n",
      "   Action Input: \"\"\"\n",
      "   import pandas as pd\n",
      "   # parse the HTML, compute 7‚Äëday avg ‚Ä¶\n",
      "   \"\"\"\n",
      "   ```\n",
      "5. After the Python code runs, the result is fed back, and the LLM finally emits:\n",
      "   ```\n",
      "   Final Answer: The 7‚Äëday average is 1,234 new cases per day.\n",
      "   ```\n",
      "\n",
      "### Why This Model Works Well\n",
      "\n",
      "* **Flexibility** ‚Äì Any callable can be turned into a tool; the planner can orchestrate arbitrary APIs, databases, or custom code.\n",
      "* **Dynamic Adaptation** ‚Äì Because the LLM re‚Äëplans after each step, the agent can react to failures, missing data, or unexpected results without hard‚Äëcoded branching logic.\n",
      "* **Memory‚ÄëDriven Reasoning** ‚Äì The memory buffer gives the LLM a persistent ‚Äúscratchpad,‚Äù enabling it to perform complex, multi‚Äëturn reasoning that feels like a human analyst.\n",
      "* **Extensibility** ‚Äì Different agent types (zero‚Äëshot, conversational, ReAct, tool‚Äëcalling) are just variations of the same planner‚Äëexecutor core, so developers can swap in custom prompts, memories, or toolsets with minimal code changes.\n",
      "\n",
      "### Bottom Line\n",
      "\n",
      "LangChain implements agents as a **closed loop** where:\n",
      "\n",
      "1. **Planner (LLM + prompt)** decides *what* tool to call next and *with what arguments*.\n",
      "2. **Executor** runs the selected tool, captures its output, and updates the agent‚Äôs **memory**.\n",
      "3. The updated memory is fed back into the planner, allowing **dynamic decision‚Äëmaking, branching, and context‚Äëaware reasoning** until a ‚Äúfinal answer‚Äù is produced.\n",
      "\n",
      "This planner‚Äëexecutor architecture is the heart of LangChain‚Äôs ability to orchestrate multi‚Äëstep, tool‚Äëdriven tasks in a way that feels like a single, coherent intelligent agent.\n",
      "\n",
      "Q: What memory capabilities does CrewAI offer, and how are they applied in CrewAI workflows?\n",
      "A: Based on the excerpts you provided, the documents focus on the kinds of workflows where CrewAI shines (market research, legal analysis, product development, coding assistants) and on its technical strengths (support for multiple LLM back‚Äëends, streaming, parallel execution, and asynchronous tool invocation).  \n",
      "\n",
      "The specific **memory capabilities** of CrewAI‚Äîsuch as whether it maintains a shared ‚Äúworking memory‚Äù across agents, supports persistent context between steps, or provides any built‚Äëin short‚Äëterm or long‚Äëterm storage‚Äîare **not described in the supplied context**. Consequently, I can‚Äôt detail how memory is implemented or applied within CrewAI workflows from the information given. If you have additional documentation that covers CrewAI‚Äôs memory model, I‚Äôd be happy to elaborate on how it‚Äôs used in multi‚Äëstep, collaborative tasks.\n",
      "\n",
      "Q: In what key ways do LangChain‚Äôs memory and agent implementations differ from CrewAI‚Äôs memory and agent approaches (e.g., architecture, flexibility, typical use cases)?\n",
      "A: **Short answer**\n",
      "\n",
      "- **Architecture** ‚Äì‚ÄØLangChain builds a *single* autonomous agent that follows a *planner‚ÄØ‚Üí‚ÄØexecutor* loop (plan a series of tool calls, then run them).  CrewAI, by contrast, is a *team‚Äëoriented* framework: a ‚Äúcrew‚Äù of **roles** (e.g., researcher, writer, editor) is defined up‚Äëfront and each role runs its own (often simpler) agent; the crew orchestrates the hand‚Äëoff of work between roles.  \n",
      "- **Memory model** ‚Äì‚ÄØLangChain‚Äôs memory lives **inside the agent‚Äôs chain** and can be swapped out (ConversationBuffer, Summary, VectorStore, etc.) to give the agent a *step‚Äëby‚Äëstep* context that persists across tool calls.  CrewAI‚Äôs memory is **role‚Äëcentric** (each role has its own private state) plus a **shared crew‚Äëwide memory** that is updated when a role finishes a task.  The crew‚Äëmemory is less about ‚Äúrecalling the last few turns‚Äù and more about ‚Äúwhat the team has produced so far‚Äù (documents, decisions, artefacts).  \n",
      "- **Flexibility vs. structure** ‚Äì‚ÄØLangChain agents are highly flexible: you can program any branching logic, conditional tool selection, and even change the plan on the fly.  CrewAI gives you a *structured workflow* (task decomposition ‚Üí role assignment ‚Üí result aggregation) and is less suited to arbitrary, on‚Äëthe‚Äëfly replanning, but it makes it easy to model human‚Äëlike collaboration patterns.  \n",
      "- **Typical use‚Äëcases** ‚Äì‚ÄØLangChain shines when you need a **single autonomous assistant** that can browse the web, call APIs, do iterative reasoning, or perform retrieval‚Äëaugmented generation.  CrewAI shines when the problem is naturally **multi‚Äëstep, multi‚Äëexpert** (e.g., market‚Äëresearch report, product‚Äëspec creation, data‚Äëpipeline design) and you want to model distinct responsibilities and a clear hand‚Äëoff of deliverables.\n",
      "\n",
      "Below is a more detailed side‚Äëby‚Äëside comparison.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Architectural differences\n",
      "\n",
      "| Aspect | LangChain | CrewAI |\n",
      "|--------|-----------|--------|\n",
      "| **Core abstraction** | *Agent* (planner‚Äëexecutor) that decides *what tool* to call next and then executes it. | *Crew* = collection of *Roles* (each a lightweight agent) plus a *Coordinator* that routes tasks. |\n",
      "| **Planning** | Explicit ‚Äúplan‚Äù step that produces a list of tool‚Äëinvocation strings (e.g., `Search[query] ‚Üí Summarize[... ]`). The plan can be re‚Äëgenerated after each step. | Planning is usually done **once** when the crew is defined: you write a **task graph** (e.g., `Researcher ‚Üí Writer ‚Üí Editor`). The crew follows that static pipeline, though you can embed a LangChain planner inside a role if you need it. |\n",
      "| **Execution flow** | Linear or branching loop: `while not done: plan ‚Üí execute ‚Üí observe ‚Üí update memory`. | Sequential role execution (or parallel if you launch several roles at once). After a role finishes, its output is stored in crew memory and the next role consumes it. |\n",
      "| **Tool integration** | Tools are wrapped as **LangChain tools**; the agent can decide *any* combination at runtime. | Tools are usually wrapped **once** and then attached to a specific role (e.g., ‚ÄúResearcher‚Äù gets a web‚Äësearch tool).  A role can still call a LangChain‚Äëwrapped tool, but the decision logic is static. |\n",
      "| **Extensibility** | Add new tools, custom planners, custom memory modules, or even custom agent loops. | Add new roles, custom role‚Äëspecific prompts, or plug a LangChain agent into a role for extra flexibility. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Memory models\n",
      "\n",
      "| Feature | LangChain Memory | CrewAI Memory |\n",
      "|---------|------------------|---------------|\n",
      "| **Granularity** | Per‚Äëagent (or per‚Äëchain). Each chain can have its own memory object. | Per‚Äërole *private* memory + a *shared crew* memory that aggregates outputs. |\n",
      "| **Typical implementations** | `ConversationBufferMemory`, `ConversationSummaryMemory`, `VectorStoreRetrieverMemory`, custom DB‚Äëbacked memory. | Simple dict‚Äëlike storage of role outputs, optional vector store for ‚Äúteam knowledge base‚Äù. |\n",
      "| **Update cadence** | After **every tool call** the agent can add the observation/result to memory, influencing the next plan step. | After a **role completes its assigned task** the result is written to crew memory; the next role reads the whole crew state. |\n",
      "| **Purpose** | Provide *step‚Äëlevel context* for the same autonomous agent (e.g., ‚Äúwhat did I just search?‚Äù). | Provide *project‚Äëlevel context*: ‚Äúwhat documents have been produced?‚Äù, ‚Äúwhat decisions were made?‚Äù, ‚Äúwhat still needs to be done?‚Äù. |\n",
      "| **Retrieval** | Can be dynamic (search the vector store each step) or static (just a buffer). | Usually a simple lookup of previous role outputs; can be upgraded to a vector store if you need semantic search across the crew‚Äôs artefacts. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Flexibility vs. Structured Collaboration\n",
      "\n",
      "| Dimension | LangChain | CrewAI |\n",
      "|-----------|-----------|--------|\n",
      "| **Dynamic replanning** | Built‚Äëin; after each observation the planner can rewrite the whole remaining plan. | Not native; the crew follows the pre‚Äëdefined role order. (You can embed a LangChain planner inside a role to get dynamic behaviour.) |\n",
      "| **Branching logic** | Easy ‚Äì the planner can add conditional branches, loops, early exits. | Limited ‚Äì branching must be expressed as multiple possible role paths ahead of time (or via conditional role execution). |\n",
      "| **Prompt engineering** | Usually a single prompt that contains the planning logic and tool‚Äëcall syntax. | Each role has its own prompt template (system + user) that reflects a specific persona or expertise. |\n",
      "| **Human‚Äëin‚Äëthe‚Äëloop** | You can pause the loop, inject a human answer, then resume. | CrewAI already models a ‚Äúhuman‚Äëlike‚Äù team; you can replace a role with a real person or a human‚Äëreview step easily. |\n",
      "| **Scalability** | Scaling usually means adding more tools or making the planner more sophisticated. | Scaling means adding more roles or parallel crews; the architecture naturally mirrors real‚Äëworld teams. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Typical use‚Äëcases\n",
      "\n",
      "| Use‚Äëcase | LangChain best fit | CrewAI best fit |\n",
      "|----------|-------------------|-----------------|\n",
      "| **Single‚Äëassistant RAG** (search‚Äëthen‚Äësummarize) | ‚úÖ LangChain‚Äôs planner + web‚Äësearch + summarizer tools. | ‚ùå Overkill ‚Äì you‚Äôd need only one role. |\n",
      "| **Autonomous workflow** (e.g., ‚Äúplan a trip, book flights, reserve hotel‚Äù) | ‚úÖ Planner can decide next API call based on previous result. | ‚úÖ If you want distinct ‚ÄúResearcher‚Äù, ‚ÄúPlanner‚Äù, ‚ÄúBooker‚Äù roles, CrewAI adds clarity. |\n",
      "| **Multi‚Äëexpert report** (market analysis ‚Üí data extraction ‚Üí writing ‚Üí editing) | Possible but you‚Äôd have to encode each expert as a separate plan step; memory gets messy. | ‚úÖ Naturally expressed as `Researcher ‚Üí Analyst ‚Üí Writer ‚Üí Editor` roles, each with its own memory. |\n",
      "| **Complex decision trees** (diagnostic tool that asks follow‚Äëup questions) | ‚úÖ Planner can loop until a condition is met. | ‚ùå Crew‚Äôs static pipeline would need a custom role that itself runs a LangChain planner. |\n",
      "| **Human‚Äëaugmented pipelines** (human reviewer after AI draft) | Possible, but you need to pause the chain. | Built‚Äëin ‚Äì you can declare a ‚ÄúHumanReviewer‚Äù role that waits for manual input. |\n",
      "| **Tool‚Äëheavy automation** (call dozens of internal APIs in a specific order) | ‚úÖ Planner can orchestrate any number of tools. | ‚úÖ If the steps map cleanly to roles (e.g., ‚ÄúFetcher‚Äù, ‚ÄúTransformer‚Äù, ‚ÄúUploader‚Äù). |\n",
      "| **Learning / teaching agents** (agent that explains its own reasoning) | ‚úÖ Memory can be queried to show step‚Äëby‚Äëstep thoughts. | ‚úÖ Crew memory can be used to show the ‚Äúteam‚Äôs‚Äù progress, but the granularity is coarser. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Putting them together ‚Äì hybrid pattern\n",
      "\n",
      "Because the two frameworks are compatible, you can get the best of both worlds:\n",
      "\n",
      "1. **Let LangChain handle the low‚Äëlevel retrieval / tool orchestration** inside a role (e.g., the ‚ÄúResearcher‚Äù role is a LangChain agent that plans a series of web‚Äësearches and summarizations).  \n",
      "2. **Let CrewAI handle the high‚Äëlevel role coordination** (deciding when the research is done, passing the summary to the ‚ÄúWriter‚Äù, then to the ‚ÄúEditor‚Äù).  \n",
      "\n",
      "This hybrid pattern is precisely what the documentation points out: *‚ÄúLangChain handles retrieval and tool wrapping, while CrewAI manages role‚Äëbased collaboration.‚Äù*  \n",
      "\n",
      "---\n",
      "\n",
      "## 6. TL;DR Summary\n",
      "\n",
      "- **Architecture** ‚Äì LangChain = one autonomous planner‚Äëexecutor agent; CrewAI = a crew of role‚Äëspecific agents coordinated by a task graph.  \n",
      "- **Memory** ‚Äì LangChain‚Äôs memory is step‚Äëlevel and interchangeable (buffer, vector store, etc.); CrewAI‚Äôs memory is role‚Äëlevel plus a shared team store that tracks deliverables.  \n",
      "- **Flexibility** ‚Äì LangChain offers dynamic replanning and branching on every step; CrewAI offers a structured, human‚Äëlike workflow that is easier to read and to map to real‚Äëworld teams.  \n",
      "- **Typical use‚Äëcases** ‚Äì LangChain excels at single‚Äëassistant, tool‚Äëdriven, adaptive tasks; CrewAI shines for multi‚Äëexpert, project‚Äëstyle pipelines where each participant has a clear responsibility.  \n",
      "\n",
      "When you need *both* dynamic tool use **and** a clear division of labor, embed a LangChain agent inside a CrewAI role and let the crew orchestrate the overall process. This gives you the planner‚Äëexecutor power of LangChain together with the role‚Äëbased collaboration and shared memory that CrewAI provides.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"‚úÖ Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003ba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
